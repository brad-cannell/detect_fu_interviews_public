---
title: "Within-Set Selection of APS Identifiers and Initial Fuzzy-Matching"
html:
  embed-resources: true
format: html
---

# ⭐️ Overview

## APS Data Background

The APS records data set was divided into 5 separate, interconnected excel files. These files are documented in the [wiki](https://github.com/brad-cannell/detect_fu_interviews_public/wiki). The primary file of interest for subject-level linkage is the "Clients.xlsx" file. This file contained 568,562 observations of 11 variables, including 378,418 values for `client_id`. 

This APS data file was cleaned/prepped for processing prior to fuzzy-matching in [a separate Quarto file](https://github.com/brad-cannell/detect_fu_interviews_public/blob/main/data_management/unique_person_identification/data_unique_person_01_within_set_aps.qmd).

## This File

This file performs selection of the identifiers that were be used to perform within-set fuzzy matching of the APS data utilizing the [fastLink package](https://cran.r-project.org/web/packages/fastLink/index.html). This was done to assess performance quality and other needs, including the potential need to divide the large data set due to computational  capacity limitations.

Variables utilized in fastLink adjusted the weighting of each difference in entries. As such, we evaluated potential combinations of variables to generate a range of posterior probabilities that would be useful in creating Unique Subject IDs, with a reasonably sized useful range of posterior probabilities for manual verification and threshold setting. Additionally, due to the size of the data, assessment of computational performance was beneficial in determining if the data would have to be "chunked" and matched iteratively, which would increase the complexity of the task.

Once the identifiers and process methods were selected, the initial fuzzy-matching was performed. Review and refinement of this output was performed in a separate file.

### Result Summary

Variables for matching were determined to be:

-   Gender: unfortunately, there was no variable indicating sex or gender of subjects in our data set. As such, no variable could be selected or used for blocking and/or exact matching.

-   Subject Name: First Name, Last Name (`client_first_name`, `client_last_name`)

    -   Many middle name values were missing in the original data set, or were otherwise widely inconsistent. Some entries utilized a single initial, an apparent nickname and/or pseudonym, or demonstrated multiple names as is practice in certain cultures. Isolation to first and last name was selected to produce the most consistent results.
    -   Partial matching was utilized for name values. This increased true-matches between observations with human-recognizeable similar names, which otherwise had significant transpositions or typos.

-   Subject Date of Birth: Year, Month, Day (`client_dob_year`,`client_dob_month`,`client_dob_day`)

    -   This weighting allowed for slight typos in one field of date of birth to have limited influence, while entirely different dates of birth had increasingly significant influence. As our concern was to account for differences made by typographical errors and/or practices to account for missing information (such as a practice of utilizing "January 1" for month and date if only birth year is known or assumed), splitting date of birth not only assisted in providing adequate weight (balanced for the two values for name, and two values for address), but also limited artificial distances that would have been created through using direct date numerical differences (i.e., weighting "1955-01-01" vs "1954-01-01" and "1930-12-05" vs "1930-11-05" equal as single-digit differences, rather than giving the year difference ~12x the significance). This was most useful for familial relations with similar names and addresses, such as married couples cohabitating.

-   Subject Address: Street Address, ZIP Code (`client_street_address`, `client_zip_code`),

    -   Using all values for address overly weighted values
    -   ZIP Code was useful for this data set, whereas it was found to be too broad in prior sets that were limited to a single state
    -   While neighbors may live on the same street, they are not likely to also have similar names or dates of birth
    -   Partial matching was utilized for Street Address values. This increased true-matches between observations with human-recognizeable similar values, which otherwise had significant transpositions or typos.
    -   Using only two values avoided overly-weighting address for familial relations with similar names and addresses, and subjects which resided at the same multi-residence location (such as an apartment complex, senior living community, or nursing home)

-   Partial Matching was utilized for Subject Name and Subject Address

    -   As mentioned in both name and address result listings, this choice increased true-matches between observations with human-recognizeable similar values, which had significant transpositions or typos.

-   A threshold of 0.70 was established for "true matches", and a threshold of 0.20 was established for "false matches"

    -   Matches between 0.20 and 0.70 (approximately 24 potential matching pairs, or 0.033% of the benchmarking chunk) would benefit from manual verification and assignment,

-   Our final fastLink output was produced with a lower threshold posterior probability of 0.20 for matches.

    -   There were 568,616 observations in the original data set, reduced to 378,604 unique combinations of `client_id` and identifiers; there were 378,418 unique IDs originally assigned by the source (`client_id`). The data's size required it be divided into three primary chunks. The first "chunk" consisted of observations with missingness in our match values, which were previously found to significantly skew matching; these resulted in chunks 3-6 for refined matching criteria based on this missingness, with chunk 7 being "unmatchable" entries due to missiness in all required identifiers. The remaining two "chunks" were created from the remaining observations through random sampling.

-   Data was exported and saved to achieve consistency, as fastLink output returns slight variations in results with each execution even with identical input. Declaration of a consistent random seed value at the start of each fastLink was utilized to minimize these differences to increase reproducibility.

    -   The fastLink output

        -   `fl_out` --\> 'data/cleaned_rds_files/unique_subject_ids/aps/aps_fl_chunk_#.rds', with # values from 1 through 6.

    -   The aps data, with new flags and modifications

        -   `aps` --\> 'data/cleaned_rds_files/unique_subject_ids/aps/aps_02_prepped_for_fl.rds'

# 📦 Load Packages and Functions

## Library Imports

```{r, warning = FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
  library(here)
  library(fastLink)
  library(janitor, include.only = "clean_names")
})
```

### Versioning

This file was created with:

-   R version 4.4.1 ("Race for Your Life").
-   tidyverse version 2.0.0, including all attached packages
-   here version 1.0.1
-   fastLink version 0.6.1
-   janitor version 2.2.0

## Functions

```{r}
# Function to reduce code repetition in informative imports of data
source(here::here("r", "informative_df_import.R"))

# Function that creates a modified version of table output, allowing
# simplified manual review of unique values in a given column or set of
# columns
source(here::here("r", "get_unique_value_summary.R"))

# Function that facilitates using fastlink to match a single data set
# without limiting the returned data
source(here::here("r", "single_df_fastlink.R"))
       
# Function that generates stacked pairs of the original data based on
# fastLink's probabilistic matching
source(here::here("r", "get_pair_data.R"))

# Function that adds a potential Unique Subject ID to pairs
source(here::here("r", "stack_ids.R"))
```

# 📥 Load Data

## APS Identifier Data

APS client data was originally in XLSX format. It had been cleaned and exported to an RDS file with 568,616 rows and 23 columns.

```{r}
aps_path <- here::here(
  "data","cleaned_rds_files", "unique_subject_ids", "aps", "aps_00_clean.rds"
  )

informative_df_import(
    "aps", aps_path, overwrite = T
  )

# 2024-11-13: APS data imported with 568,616 rows and 23 columns.
# Data last modified on OneDrive: 2024-11-01 16:59:03
```

## Flag of Invalid Many-to-Many for Client-Case Relationship

As all cases should only be associated with a single client, we flagged all observations that contained one of the 174 observations with a `case_id` value that was associated with more than one `client_id`.

```{r}
multiple_client_ids <- unique(pull(
  aps |>
    dplyr::select(case_id, client_id) |>
    dplyr::distinct() |>
    dplyr::mutate(flag = ifelse( 
      (duplicated(case_id)|duplicated(case_id, fromLast = TRUE)),T, F)
      ) |>
  dplyr::filter(flag) |>
  dplyr::select(case_id)
  ))

aps <- aps |>
  dplyr::mutate(flag_mult_clients = ifelse(
    case_id %in% multiple_client_ids, T, F
  ))

length(multiple_client_ids)
rm(multiple_client_ids)

# 174
```

## Conversion of Subject ZIP Code to Numeric

Subject Zip Code data, was coded as character to preserve leading zeros and formatting. For the purposes of numeric matching with fastLink, it was converted to a numeric.

```{r}
aps$client_zip_code <- as.numeric(aps$client_zip_code)
```

## Minimization of Repeat Values

As our matching would take place using the APS provided `client_id` values in our final map, we reduced our data set to unique observations tied to this identifier, rather than any others. This reduced our data set to 378,604 unique combinations of identifiers and `client_id` values (33.42% reduction).

```{r}
aps <- aps |>
    dplyr::select(-case_id) |>
    distinct()

format(nrow(aps), big.mark = ',')
# [1] 378,604
```

## Addition of Row ID value

We added a column indicating row number for the observations in the trimmed APS data, ensuring we would be able to trace back to the exact row if we required additional information in our review.

```{r}
aps <- aps |>
  dplyr::mutate(
    aps_row = dplyr::row_number()
  )
```

## Packing of Data

We packed the APS data with additional values, for simplicity as we explored potential fastLink outputs. The assignment of the list of ID variables would be made within each iteration of fuzzy-matching. 

```{r}
aps_pack <- list()
aps_pack$df <- aps
aps_pack$suffix <- 'aps'

## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", "client_street_address",
  "client_city", "client_state"
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )
```

## Random Seed

As fastLink utilized some randomness in its processing, we set a random seed, to be declared again at the start of any cell that utilized fastLink, to attempt to maximize reproducibility.

```{r}
doc_seed <- 42
set.seed(doc_seed)
```

# Determination of Variables

As there were 378,418 unique client_id values in our data set, we expected to have at least 378,418 pairs with perfect matches (as each row is compared to each row, so each row should at minimum be a perfect match with itself).

## Initial Attempt

Initial attempts to link records matched based on 9 identifying variables: Subject Name (First and Last), Subject DOB (Year, Month, and Day), and Subject Address (Street Address, City, State, and Zip).

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", "client_street_address",
  "client_city", "client_state"
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )
```


```{r, eval = F}
# Set seed
set.seed(doc_seed)

## fastLink
fl_out <- single_df_fastLink(aps, str_vars, num_vars)
```

This initial attempt to process using these variables for the entire data set was abandoned, as progress stalled after 30 minutes.

A secondary attempt, to evaluate the variables, was made on a subset of the APS data. We utilized random sampling to select 50% of our original data set to attempt processing again.

```{r, eval = F}
# Set seed
set.seed(doc_seed)

## Generating subset
sample_ratio <- 0.50

flags <- sample(c(
      rep(TRUE,nrow(aps)*sample_ratio), 
      rep(FALSE,nrow(aps)*(1-sample_ratio))
      ), replace = FALSE
    )

### Add an additional flag if the vector is a value short, due to rounding
if (length(flags) == nrow(aps) - 1) {
  flags <- c(flags, F)
}

aps$flags <- flags

subset <- aps |>
  dplyr::filter(flags)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(subset, str_vars, num_vars)
```

However, this attempt to use these variables for 50% of the data set was abandoned, as progress again stalled after 30 minutes.

A third attempt, to evaluate the variables, was made on a subset of the APS data. We utilized random sampling to select 10% of our original data set to attempt processing again.

```{r, eval = F}
# Set seed
set.seed(doc_seed)

## Generating subset
sample_ratio <- 0.10

flags <- sample(c(
      rep(TRUE,nrow(aps)*sample_ratio), 
      rep(FALSE,nrow(aps)*(1-sample_ratio))
      ), replace = FALSE
    )

### Add an additional flag if the vector is a value short, due to rounding
if (length(flags) == nrow(aps) - 1) {
  flags <- c(flags, F)
}

aps$flags <- flags

subset <- aps |>
  dplyr::filter(flags)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(subset, str_vars, num_vars)
```

Unfortunately, this attempt to use these variables for 10% of the data set was abandoned, as progress again stalled after 30 minutes.

A fourth and final attempt to evaluate the variables was made on a subset of the APS data. We utilized random sampling to select 5% of our original data set to attempt processing again.

```{r}
# Set seed
set.seed(doc_seed)

## Generating subset
sample_ratio <- 0.05

flags <- sample(c(
      rep(TRUE,nrow(aps)*sample_ratio), 
      rep(FALSE,nrow(aps)*(1-sample_ratio))
      ), replace = FALSE
    )

### Add an additional flag if the vector is a value short, due to rounding
if (length(flags) == nrow(aps) - 1) {
  flags <- c(flags, F)
}

aps$flags <- flags

subset <- aps |>
  dplyr::filter(flags)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(subset, str_vars, num_vars)
```

While calculations managed to complete, the posterior probabilities generated in pairings were likely to demonstrate significant errors as the EM algorithm was unable to converge despite 5000 iterations. With this concern in mind, we generated our stacked pairs and IDs to evaluate the output.

```{r}
test_stack <- get_pair_data(
  subset_pack, subset_pack, c(str_vars, num_vars), fl_out
  )

test_id <- stack_ids(subset_pack, subset_pack, test_stack)
```

### Output Summary

In addition to the relatively prodigious processing time (\> 45 minutes), this produced a large number of potential pairs (146,262 pairs, with 18,930 rows in the data chunk).

Posterior probabilities ranged from 0.8583249997377 to 1.0. 16,873 entries had posterior probabilities of 1.0, indicating a perfect match.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# Any rows without a posterior probability?
format(sum(is.na(test_stack$posterior_probability)), big.mark=',')

# How many unique pairs?
format(nrow(test_stack), big.mark=',')

# How many rows in the original data set that was processed?
format(nrow(subset), big.mark = ',')

# How many pairs with a posterior probability of 1 (perfect match)?
format(sum(test_stack$posterior_probability == 1), big.mark=',')

# Get summary stats for the posterior probabilities, with min and max as focus
format(summary(test_stack$posterior_probability), big.mark=',')
```

Cursory examination for observations with posteriors just below 1.0 included a large number of mismatches and duplicates. Many mismatches included transposed names, relatively common first or last names, individuals that appeared to be family members such as siblings and spouses, and individuals residing at in a multi-residence location such as apartment complexes, senior living facilities, and nursing homes. Similarly, pairs at lower values of posterior probability indicated a large number of actual matches intermixed with false matches.

A total of 18,844,922 observations were generated in the ID-containing data set. There were 146,262 unique subject IDs created.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows are in the ID generation set?
format(nrow(test_id), big.mark=',')

# How many unique subject ID values were created?
format(length(unique(test_id$id)), big.mark=',')
```

### Duplicate Checking

We found 18,071,855 observations with repeat `aps_row` numbers for the "first" member of the pair (excluding observations where the `aps_row` for both rows in the pair were identical).

```{r}
duplicates <- test_id |>
  dplyr::filter(
    aps_row_a != aps_row_b & (
      duplicated(aps_row_a) | duplicated(aps_row_a, fromLast = T)
      )
    )

format(nrow(duplicates), big.mark = ",")
# [1] "18,071,855"
```

We found no duplicates that were missing a aps_row value, and we lost 2,496 observations based on aps_row numbers.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows have a missing value for one of the row numbers?
format(
  sum(is.na(duplicates$aps_row_a)|is.na(duplicates$aps_row_b)), 
  big.mark = ","
  )

# Are there an equal number of unique row values as rows in the original set?
# (i.e., were any observations lost?)
length(unique(c(duplicates$aps_row_a,duplicates$aps_row_b))) == nrow(subset)

# How many observations were lost? (if any)
format(length(setdiff(
  subset$aps_row, 
  unique(c(duplicates$aps_row_a,duplicates$aps_row_b))
  )), big.mark = ',')
```

We flagged all observations with a matching `aps_row` or `id` from the duplicated observations for a manual review. We found 18,843,802 observations, almost the entirety of the pair ID data set, that were flagged for this reason.

```{r}
test_id$flag <- FALSE

test_id <- test_id |>
  dplyr::mutate(flag = dplyr::case_when(
    id %in% duplicates$id ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_a)|
        (aps_row_b %in% duplicates$aps_row_a)
      ) ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_b)|
        (aps_row_b %in% duplicates$aps_row_b)
      ) ~ TRUE,
    TRUE ~ flag
  ))

get_unique_value_summary(test_id,"flag")
```

In checking these values, we saw that many of them appeared to be caused by a prodigious number of false matches, similar to the cursory examination.

```{r}
checking <- test_id |>
  dplyr::filter(flag) |>
  dplyr::group_by(id) |>
  dplyr::arrange(posterior_probability)
```

## Reduction of Breadth for Address Fields

Initial attempts for linkage revealed that address values were disproportionately significant, likely due to the number of address fields used in matching. This appeared to cause individuals that lived within the same multi-residence facility (including senior living facilities, nursing homes, and apartment complexes) to have high posterior probabilities, despite significantly different Names and Dates of Birth. For this attempt at refining the process, all address fields except zip code were excluded.

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name"
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )
```


```{r, eval = F}
# Set seed
set.seed(doc_seed)

## fastLink
fl_out <- single_df_fastLink(aps, str_vars, num_vars)
```

This initial attempt to process using these variables for the entire data set was abandoned, as progress stalled after 45 minutes.

A secondary attempt, to evaluate the variables, was made on a subset of the APS data. We utilized random sampling to select 10% of our original data set to attempt processing again.

```{r}
# Set seed
set.seed(doc_seed)

## Generating subset
sample_ratio <- 0.10

flags <- sample(c(
      rep(TRUE,nrow(aps)*sample_ratio), 
      rep(FALSE,nrow(aps)*(1-sample_ratio))
      ), replace = FALSE
    )

### Add an additional flag if the vector is a value short, due to rounding
if (length(flags) == nrow(aps) - 1) {
  flags <- c(flags, F)
}

aps$flags <- flags

subset <- aps |>
  dplyr::filter(flags)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(subset, str_vars, num_vars)
```

Calculations managed to complete within 45 minutes without stalls or failure to achieve convergence. We generated our stacked pairs and IDs to evaluate the output.

```{r}
test_stack <- get_pair_data(
  subset_pack, subset_pack, c(str_vars, num_vars), fl_out
  )

test_id <- stack_ids(subset_pack, subset_pack, test_stack)
```

### Output Summary 

While this processing continued to have a relatively prodigious processing time (\> 30 minutes), it produced a far more reasonable number of potential pairs (35,174 pairs, with 37,860 rows in the data chunk).

Posterior probabilities ranged from 0.8615369858805 to 0.9999994875504. There were no entries that had posterior probabilities of 1.0, indicating a perfect match, despite the fact the data set was matching internally and thus should have had such values.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# Any rows without a posterior probability?
format(sum(is.na(test_stack$posterior_probability)), big.mark=',')

# How many unique pairs?
format(nrow(test_stack), big.mark=',')

# How many rows in the original data set that was processed?
format(nrow(subset), big.mark = ',')

# How many pairs with a posterior probability of 1 (perfect match)?
format(sum(test_stack$posterior_probability == 1), big.mark=',')

# Get summary stats for the posterior probabilities, with min and max as focus
format(summary(test_stack$posterior_probability), big.mark=',')
```

A total of 35,952 observations were generated in the ID-containing data set. There were 35,174 unique subject IDs created.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows are in the ID generation set?
format(nrow(test_id), big.mark=',')

# How many unique subject ID values were created?
format(length(unique(test_id$id)), big.mark=',')
```

### Duplicate Checking

We found 587 observations with repeat `aps_row` numbers for the "first" member of the pair (excluding observations where the `aps_row` for both rows in the pair were identical).

```{r}
duplicates <- test_id |>
  dplyr::filter(
    aps_row_a != aps_row_b & (
      duplicated(aps_row_a) | duplicated(aps_row_a, fromLast = T)
      )
    )

format(nrow(duplicates), big.mark = ",")
# [1] "587"
```

We found no duplicates that were missing an `aps_row` value, and we lost 37,495 observations based on aps_row numbers.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows have a missing value for one of the row numbers?
format(
  sum(is.na(duplicates$aps_row_a)|is.na(duplicates$aps_row_b)), 
  big.mark = ","
  )

# Are there an equal number of unique row values as rows in the original set?
# (i.e., were any observations lost?)
length(unique(c(duplicates$aps_row_a,duplicates$aps_row_b))) == nrow(subset)

# How many observations were lost? (if any)
format(length(setdiff(
  subset$aps_row, 
  unique(c(duplicates$aps_row_a,duplicates$aps_row_b))
  )), big.mark = ',')
```

We flagged all observations with a matching `aps_row` or `id` from the duplicated observations for a manual review. We found 1,331 observations, a reasonably small portion of the pair ID data set, that were flagged for this reason.

```{r}
test_id$flag <- FALSE

test_id <- test_id |>
  dplyr::mutate(flag = dplyr::case_when(
    id %in% duplicates$id ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_a)|
        (aps_row_b %in% duplicates$aps_row_a)
      ) ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_b)|
        (aps_row_b %in% duplicates$aps_row_b)
      ) ~ TRUE,
    TRUE ~ flag
  ))

get_unique_value_summary(test_id,"flag")
```

In checking these values, we saw that many of them appeared to be caused by a prodigious number of false matches, particularly between potentially cohabitating spouses or other familial relations.

```{r}
checking <- test_id |>
  dplyr::filter(flag) |>
  dplyr::group_by(id) |>
  dplyr::arrange(posterior_probability)
```

## Reintroduction of Street Address

To account for matching shortfalls in the prior variable selection, we reintroduced the street address value as a match parameter, in effort to make any address-based matches more robust.

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", 'client_street_address'
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )
```


```{r, eval = F}
# Set seed
set.seed(doc_seed)

## fastLink
fl_out <- single_df_fastLink(aps, str_vars, num_vars)
```

This initial attempt to process using these variables for the entire data set was abandoned, as progress stalled after 45 minutes.

A secondary attempt, to evaluate the variables, was made on a subset of the APS data. We utilized random sampling to select 10% of our original data set to attempt processing again.

```{r}
# Set seed
set.seed(doc_seed)

## Generating subset
sample_ratio <- 0.10

flags <- sample(c(
      rep(TRUE,nrow(aps)*sample_ratio), 
      rep(FALSE,nrow(aps)*(1-sample_ratio))
      ), replace = FALSE
    )

### Add an additional flag if the vector is a value short, due to rounding
if (length(flags) == nrow(aps) - 1) {
  flags <- c(flags, F)
}

aps$flags <- flags

subset <- aps |>
  dplyr::filter(flags)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(subset, str_vars, num_vars)
```

Calculations managed to complete within 45 minutes without stalls or failure to achieve convergence. We generated our stacked pairs and IDs to evaluate the output.

```{r}
test_stack <- get_pair_data(
  subset_pack, subset_pack, c(str_vars, num_vars), fl_out
  )

test_id <- stack_ids(subset_pack, subset_pack, test_stack)
```

### Output Summary

While this processing continued to have a relatively prodigious processing time (\> 30 minutes), it produced a more reasonable number of pairs (43,506 pairs, with 37,860 rows in the data chunk).

Posterior probabilities ranged from 0.8576523040020 to 0.9999999999918. There were no entries that had posterior probabilities of 1.0, indicating a perfect match, despite the fact the data set was matching internally and thus should have had such values.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# Any rows without a posterior probability?
format(sum(is.na(test_stack$posterior_probability)), big.mark=',')

# How many unique pairs?
format(nrow(test_stack), big.mark=',')

# How many rows in the original data set that was processed?
format(nrow(subset), big.mark = ',')

# How many pairs with a posterior probability of 1 (perfect match)?
format(sum(test_stack$posterior_probability == 1), big.mark=',')

# Get summary stats for the posterior probabilities, with min and max as focus
format(summary(test_stack$posterior_probability), big.mark=',')
```

A total of 189,688 observations were generated in the ID-containing data set. There were 43,506 unique subject IDs created.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows are in the ID generation set?
format(nrow(test_id), big.mark=',')

# How many unique subject ID values were created?
format(length(unique(test_id$id)), big.mark=',')
```

### Duplicate Checking 

We found 129,488 observations with repeat `aps_row` numbers for the "first" member of the pair (excluding observations where the `aps_row` for both rows in the pair were identical).

```{r}
duplicates <- test_id |>
  dplyr::filter(
    aps_row_a != aps_row_b & (
      duplicated(aps_row_a) | duplicated(aps_row_a, fromLast = T)
      )
    )

format(nrow(duplicates), big.mark = ",")
#[1] "129,488"
```

We found no duplicates that were missing an `aps_row` value, and we lost 29,905 observations based on aps_row numbers.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows have a missing value for one of the row numbers?
format(
  sum(is.na(duplicates$aps_row_a)|is.na(duplicates$aps_row_b)), 
  big.mark = ","
  )

# Are there an equal number of unique row values as rows in the original set?
# (i.e., were any observations lost?)
length(unique(c(duplicates$aps_row_a,duplicates$aps_row_b))) == nrow(subset)

# How many observations were lost? (if any)
format(length(setdiff(
  subset$aps_row, 
  unique(c(duplicates$aps_row_a,duplicates$aps_row_b))
  )), big.mark = ',')
```

We flagged all observations with a matching `aps_row` or `id` from the duplicated observations for a manual review. We found 162,665 observations, the vast majority of the pair ID data set, that were flagged for this reason.

```{r}
test_id$flag <- FALSE

test_id <- test_id |>
  dplyr::mutate(flag = dplyr::case_when(
    id %in% duplicates$id ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_a)|
        (aps_row_b %in% duplicates$aps_row_a)
      ) ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_b)|
        (aps_row_b %in% duplicates$aps_row_b)
      ) ~ TRUE,
    TRUE ~ flag
  ))

get_unique_value_summary(test_id,"flag")
```

In checking these values, the lower range appeared to have a mix of false and true matches. False matches appeared to be primarily among individuals with highly similar (typically highly common) name values. However, this overall appeared to produce a balanced result.

```{r}
checking <- test_id |>
  dplyr::filter(flag) |>
  dplyr::group_by(id) |>
  dplyr::arrange(posterior_probability)
```

## Introduction of Partial Matching

In similar prior tasks, partial matching was found to be valuable to balance the effects of both missingness and trends in string distance differences, such as highly shortened nicknames (e.g. "Josephine" to "Jo"). We thus tested using partial matching on all string variables.

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", 'client_street_address'
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )
```


```{r, eval = F}
# Set seed
set.seed(doc_seed)

## fastLink
fl_out <- single_df_fastLink(
  aps, str_vars, num_vars, partial.match = str_vars
  )
```

This initial attempt to process using these variables for the entire data set was abandoned, as progress stalled after 45 minutes.

A secondary attempt, to evaluate the variables, was made on a subset of the APS data. We utilized random sampling to select 10% of our original data set to attempt processing again.

```{r}
# Set seed
set.seed(doc_seed)

## Generating subset
sample_ratio <- 0.10

flags <- sample(c(
      rep(TRUE,nrow(aps)*sample_ratio), 
      rep(FALSE,nrow(aps)*(1-sample_ratio))
      ), replace = FALSE
    )

### Add an additional flag if the vector is a value short, due to rounding
if (length(flags) == nrow(aps) - 1) {
  flags <- c(flags, F)
}

aps$flags <- flags

subset <- aps |>
  dplyr::filter(flags)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars
  )
```

Calculations managed to complete within 45 minutes without stalls or failure to achieve convergence. We generated our stacked pairs and IDs to evaluate the output.

```{r}
test_stack <- get_pair_data(
  subset_pack, subset_pack, c(str_vars, num_vars), fl_out
  )

test_id <- stack_ids(subset_pack, subset_pack, test_stack)
```

### Output Summary

While this processing continued to have a relatively prodigious processing time (\> 30 minutes), it notably produced significantly fewer pairs than processing the same variables without partial matching (35,175 pairs, with 37,860 rows in the data chunk).

Posterior probabilities ranged from 0.8738811221575 to 0.9999999999982. There were no entries that had posterior probabilities of 1.0, indicating a perfect match, despite the fact the data set was matching internally and thus should have had such values.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# Any rows without a posterior probability?
format(sum(is.na(test_stack$posterior_probability)), big.mark=',')

# How many unique pairs?
format(nrow(test_stack), big.mark=',')

# How many rows in the original data set that was processed?
format(nrow(subset), big.mark = ',')

# How many pairs with a posterior probability of 1 (perfect match)?
format(sum(test_stack$posterior_probability == 1), big.mark=',')

# Get summary stats for the posterior probabilities, with min and max as focus
format(summary(test_stack$posterior_probability), big.mark=',')
```

A total of 35,923 observations were generated in the ID-containing data set. There were 35,175 unique subject IDs created.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows are in the ID generation set?
format(nrow(test_id), big.mark=',')

# How many unique subject ID values were created?
format(length(unique(test_id$id)), big.mark=',')
```

### Duplicate Checking

We found 563 observations with repeat `aps_row` numbers for the "first" member of the pair (excluding observations where the `aps_row` for both rows in the pair were identical).

```{r}
duplicates <- test_id |>
  dplyr::filter(
    aps_row_a != aps_row_b & (
      duplicated(aps_row_a) | duplicated(aps_row_a, fromLast = T)
      )
    )

format(nrow(duplicates), big.mark = ",")
#[1] "563"
```

We found no duplicates that were missing an `aps_row` value, and we lost 37,500 observations based on `aps_row` numbers.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows have a missing value for one of the row numbers?
format(
  sum(is.na(duplicates$aps_row_a)|is.na(duplicates$aps_row_b)), 
  big.mark = ","
  )

# Are there an equal number of unique row values as rows in the original set?
# (i.e., were any observations lost?)
length(unique(c(duplicates$aps_row_a,duplicates$aps_row_b))) == nrow(subset)

# How many observations were lost? (if any)
format(length(setdiff(
  subset$aps_row, 
  unique(c(duplicates$aps_row_a,duplicates$aps_row_b))
  )), big.mark = ',')
```

We flagged all observations with a matching `aps_row` or `id` from the duplicated observations for a manual review. We found 1,291 observations, approximately half of the pair ID data set, that were flagged for this reason.

```{r}
test_id$flag <- FALSE

test_id <- test_id |>
  dplyr::mutate(flag = dplyr::case_when(
    id %in% duplicates$id ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_a)|
        (aps_row_b %in% duplicates$aps_row_a)
      ) ~ TRUE,
    (
      (aps_row_a %in% duplicates$aps_row_b)|
        (aps_row_b %in% duplicates$aps_row_b)
      ) ~ TRUE,
    TRUE ~ flag
  ))

get_unique_value_summary(test_id,"flag")
```

In checking these values, these matches appeared to be fairly trustworthy, even at the lower range of posterior probabilities included. There were a few values in the lower range which appeared to possibly be false or questionable matches, largely between individuals with relatively common names. However, this overall appeared to produce our highest-quality result thus far.

```{r}
checking <- test_id |>
  dplyr::filter(flag) |>
  dplyr::group_by(id) |>
  dplyr::arrange(posterior_probability)
```

# Determination of Posterior Probabilities 

To review the range of posterior probabilities that would provide a maximal return for manual review, we had to examine our results in greater detail, across a broader range than the default lower threshold.

### Identification of Missing Patterns in Matching Variables

Because our fuzzy-matching relied upon several fields, and missingness would
skew results in fuzzy-matching, we determined the missingness patterns in
these variables.

-   29,729 rows were missing both name fields. 30,460 (additional 731 rows) were missing only one of the two name fields
-   36,488 rows were missing both address fields. 41,447 (additional 4,959) were missing only one of the two address fields
-   28,599 rows were missing date of birth entirely (present or absent in data set).
-   A total of 28,599 rows, approximately 11.32% of the data set, were missing values for any of our matching variables

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# Number missing both name fields
format(nrow(
  aps |>
  dplyr::filter(is.na(client_first_name) & is.na(client_last_name))
  ), 
  big.mark = ',')

# Number missing either name field
format(nrow(
  aps |>
  dplyr::filter(is.na(client_first_name) | is.na(client_last_name))
  ), 
  big.mark = ',')

# Number missing both address fields
format(nrow(
  aps |>
  dplyr::filter(is.na(client_street_address) & is.na(client_zip_code))
  ), 
  big.mark = ',')

# Number missing either address field
format(nrow(
  aps |>
  dplyr::filter(is.na(client_street_address) | is.na(client_zip_code))
  ), 
  big.mark = ',')

# Number missing DOB
format(nrow(
  aps |>
  dplyr::filter(is.na(client_date_of_birth))
  ), 
  big.mark = ',')


# Number missing any of our match variables
missing_vals <- aps |>
  dplyr::filter(
    is.na(client_first_name) | is.na(client_last_name) | 
      is.na(client_date_of_birth) |
      is.na(client_street_address) | is.na(client_zip_code)
    )

# % of data set missing any of our match variables
paste0(format(
  nrow(missing_vals)*100/nrow(aps)
  , digits = 4), "%")
```

Due to the small proportion, but large number, of these rows, we made the decision to process these rows in small chunks to reduce the influence of missingness on our larger matching chunks. As such, we flagged these issues for the remainder of benchmarking and initial processing.

```{r}
aps <- aps |>
  dplyr::mutate(
    flag_issues = ifelse(
      ( is.na(client_date_of_birth) | is.na(client_first_name) | 
          is.na(client_last_name) | is.na(client_street_address) | 
          is.na(client_zip_code) ), 
      T, 
      F
      )
    )
```


##fastLink

We utilized fastLink and requested all matches with posterior probabilities greater than 0.001 for a 20% subset of our data set.

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", 'client_street_address'
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )
```


```{r, eval = F}
# Set seed
set.seed(doc_seed)

## Generating subset
sample_ratio <- 0.20

chunks <- sample(c(
      rep(TRUE,nrow(aps)*sample_ratio), 
      rep(FALSE,nrow(aps)*(1-sample_ratio))
      ), replace = FALSE
    )

### Add an additional 'chunk' F if the vector is a value short due to rounding
if (length(chunks) == nrow(aps) - 1) {
  chunks <- c(chunks, F)
}

aps$chunks <- chunks

subset <- aps |>
  dplyr::filter(chunks) |>
  dplyr::filter(!flag_issues)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars, return.all = TRUE
  )
```

We used the fastLink output to generate our stacked pairs for inspection, and a version of the data set with unique IDs based on these matches.

```{r}
test_stack <- get_pair_data(
  subset_pack, subset_pack, c(str_vars, num_vars), fl_out
  )

test_id <- stack_ids(subset_pack, subset_pack, test_stack)
```

## Output Summary

While this processing continued to have a relatively prodigious processing time (\> 30 minutes), it notably produced significantly similar results to initial test of partial matching (68,227 pairs, with 67,115 rows in the data chunk).

Posterior probabilities ranged from 0.001567767301893 to 0.999999999996394. There were no entries that had posterior probabilities of 1.0, indicating a perfect match, despite the fact the data set was matching internally and thus should have had such values.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# Any rows without a posterior probability?
format(sum(is.na(test_stack$posterior_probability)), big.mark=',')

# How many unique pairs?
format(nrow(test_stack), big.mark=',')

# How many rows in the original data set that was processed?
format(nrow(subset), big.mark = ',')

# How many pairs with a posterior probability of 1 (perfect match)?
format(sum(test_stack$posterior_probability == 1), big.mark=',')

# Get summary stats for the posterior probabilities, with min and max as focus
format(summary(test_stack$posterior_probability), big.mark=',')
```

A total of 72,911 observations were generated in the ID-containing data set. There were 68,227 unique subject IDs created.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

# How many rows are in the ID generation set?
format(nrow(test_id), big.mark=',')

# How many unique subject ID values were created?
format(length(unique(test_id$id)), big.mark=',')
```

### Evaluation of Posterior Probability Ranges

#### 0.90 - 0.95

The 100 pairs (100 IDs) with posteriors between 0.90 - 0.95 were examined.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.90,0.95)))

nrow(checking)
length(unique(checking$id))
```

All pairs appeared to be valid.

#### 0.80 - 0.85

There were no pairs with posteriors between 0.80 - 0.85.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.80,0.85)))

nrow(checking)
length(unique(checking$id))
```

#### 0.75 - 0.80

There were no pairs with posteriors between 0.75 - 0.80.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.75,0.80)))

nrow(checking)
length(unique(checking$id))
```

#### 0.70 - 0.75

The 9 pairs (9 IDs) with posteriors between 0.70 - 0.75 were examined.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.70,0.75)))

nrow(checking)
length(unique(checking$id))
```

All appeared to be true matches, wherein one observation would utilize a nickname and both observations had separate addresses. Due to the unique names and identical birthdates, these were believed to be true pairs.

#### 0.65 - 0.70

There were no pairs with posteriors between 0.65 - 0.70.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.65,0.70)))

nrow(checking)
length(unique(checking$id))
```

#### 0.60 - 0.65

There were no pairs with posteriors between 0.60 - 0.65.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.60,0.65)))

nrow(checking)
length(unique(checking$id))
```

#### 0.55 - 0.60

There were no pairs with a posterior probability between 0.55 - 0.60.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.55,0.60)))

nrow(checking)
length(unique(checking$id))
```

#### 0.50 - 0.55

There were no pairs with posteriors between 0.50 - 0.55.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.50,0.55)))

nrow(checking)
length(unique(checking$id))
```

These pairs appeared to be a mix of true, false, and questionable matches.

#### 0.45 - 0.50

There were no pairs with a posterior probability between 0.45 - 0.50.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.45,0.50)))

nrow(checking)
length(unique(checking$id))
```

#### 0.40 - 0.45

The 6 pairs (6 IDs) with posteriors between 0.40 - 0.45 were examined.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.40,0.45)))

nrow(checking)
length(unique(checking$id))
```

All pairs represented questionable matches of relatively common hispanic names, similar birth years and days, but differing birth months and street addresses.

#### 0.35 - 0.40

There were no pairs with a posterior probability between 0.35 - 0.40

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.35,0.40)))

nrow(checking)
length(unique(checking$id))
```

#### 0.30 - 0.35

There were no pairs with a posterior probability between 0.30 - 0.35

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.30,0.35)))

nrow(checking)
length(unique(checking$id))
```

#### 0.25 - 0.30

The 18 pairs (18 IDs) with posteriors between 0.25 - 0.30 were examined

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.25,0.30)))

nrow(checking)
length(unique(checking$id))
```

All pairs appeared to be questionable but potentially valid matches between individuals with similar highly unique names, but differing at a single element of birth date and in street address.

#### 0.20 - 0.25

There were no pairs with a posterior probability between 0.20 - 0.25

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.20,0.25)))

nrow(checking)
length(unique(checking$id))
```

#### 0.15 - 0.20

The 6 pairs (6 IDs) with posteriors between 0.15 - 0.20 were examined.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.15,0.20)))

nrow(checking)
length(unique(checking$id))
```

All pairs appeared to be false matches between individuals with similar common name values, but differing dates of birth (except a single component match) and street address.

#### 0.10 - 0.15

There were no pairs with posteriors between 0.10 - 0.15.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.10,0.15)))

nrow(checking)
length(unique(checking$id))
```

#### 0.05 - 0.10

The 560 pairs (541 IDs) with posteriors between 0.05 - 0.10 were examined

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.05,0.10)))

nrow(checking)
length(unique(checking$id))
```

None of these pairs appeared to be true matches. 

#### Summary (0.20 - 0.70)

There were 24 pairs (24 unique IDs) with reverse probabilities within the range of 0.20 - 0.70, which would require manual verification of proper pairing and ID. Despite how wide this range was, only 0.033% of potential pairs were likely to benefit from manual verification.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================
checking <- test_id |>
  dplyr::filter( (dplyr::between(posterior_probability,0.20,0.70)))

nrow(checking)
length(unique(checking$id))
paste0(format(nrow(checking)*100 / nrow(test_id), digits = 4),"%")
```

# Determination of Chunk Sizing 

As our data set was fairly large, and previous examination had been forced to utilized chunking to avoid stalls, we sought to identify the largest possible chunk size to minimize the need for iterative cross-matching of chunks to create a unified set. Processing would be performed with a return lower value threshold of 0.20, as determined through our posterior probability exploration. 

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", 'client_street_address'
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )
```

We first attempted processing 50% of the data set, which would result in 2 chunks for full processing in addition to smaller chunks processed to account for missingness.

```{r, eval = F}
# Set seed
set.seed(doc_seed)

## Generating subset
aps <- aps |>
  dplyr::mutate(
    chunks = ifelse(flag_issues, 3, sample(c(1,2), prob = c(0.5, 0.5)))
  )

subset <- aps |>
  dplyr::filter(!flag_issues & chunks == 1)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_1'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars, threshold.match = 0.20
  )
```

This processing was successful, though this processing took over 1 hour to complete. A bottleneck was clear at the final stage of processing, which required 100% of CPU capacity. Due to this limitation, this was determined to be the maximal chunk size. This chunk would be utilized as the first of our five chunks for later processing.

## Processing of Chunks

### Assigning Chunks to Rows

We adjusted row chunk assignments to match our smaller chunks: 

-   3: Missing either name field only
-   4: Missing either address field only
-   5: Missing DOB only
-   6: Remaining rows with missing values
-   7: Missing all values (unmatchable, excluded from processing)

```{r}
aps <- aps |>
  dplyr::mutate(
    chunks = dplyr::case_when(
      ( is.na(client_date_of_birth) & is.na(client_first_name) & 
          is.na(client_last_name) & is.na(client_street_address) & 
          is.na(client_zip_code) ) ~ 7,
      ( is.na(client_first_name) | is.na(client_last_name) ) &
        !(is.na(client_date_of_birth) | is.na(client_street_address) |
            is.na(client_zip_code)) ~ 3,
      ( is.na(client_street_address) | is.na(client_zip_code) ) &
        !(is.na(client_first_name) | is.na(client_last_name) | 
            is.na(client_date_of_birth) ) ~ 4,
      ( is.na(client_date_of_birth) ) &
        !(is.na(client_first_name) | is.na(client_last_name) | 
            is.na(client_street_address) | is.na(client_zip_code) ) ~ 5,
      ( is.na(client_date_of_birth) | is.na(client_first_name) | 
          is.na(client_last_name) | is.na(client_street_address) | 
          is.na(client_zip_code) ) ~ 6,
      TRUE ~ chunks
    )
  )
```

### Chunk 1

#### 💾 Save and Export Data

We exported our data, for further use.

```{r}
saveRDS(
  fl_out,
  here::here(
    "data", "cleaned_rds_files", "unique_subject_ids", "aps", 
    "aps_01_fl_chunk_1.rds"
    )
)

```

### Chunk 2

```{r}
# Set seed
set.seed(doc_seed)

subset <- aps |>
  dplyr::filter(chunks == 2)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_2'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars, threshold.match = 0.20
  )
```

#### 💾 Save and Export Data

We exported our data, for further use.

```{r}
saveRDS(
  fl_out,
  here::here(
    "data", "cleaned_rds_files", "unique_subject_ids", "aps", 
    "aps_01_fl_chunk_2.rds"
    )
)

```

### Missingness Chunks

The remaining chunks were separated for processing based on which values they displayed missingness within.

#### Chunk 3

Chunk 3 represented rows which had missingness in name fields. As less than half of the 1,377 rows had one of our two target name values (only 463 with a first name and 67 with a last name), we omitted names from matching in this set to avoid the undue influence of missingness in these values.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

checking <- aps |>
  filter(chunks == 3)

# How many rows are in this subset?
format(nrow(
  checking
  ), big.mark = ',')

# How many rows have first name?
format(nrow(
  checking |>
    filter(!is.na(client_first_name))
  ), big.mark = ',')

# How many rows have last name?
format(nrow(
  checking |>
    filter(!is.na(client_last_name))
  ), big.mark = ',')

# [1] "1,377"
# [1] "463"
# [1] "67"
```


```{r}
## Defining Target Variables
str_vars <- c(
  'client_street_address'
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )

# Set seed
set.seed(doc_seed)

subset <- aps |>
  dplyr::filter(chunks == 3)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_3'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars, threshold.match = 0.20
  )
```

##### 💾 Save and Export Data

We exported our data, for further use.

```{r}
saveRDS(
  fl_out,
  here::here(
    "data", "cleaned_rds_files", "unique_subject_ids", "aps", 
    "aps_01_fl_chunk_3.rds"
    )
)

```

#### Chunk 4

Chunk 4 represented rows which had missingness in address fields. As less than half of the 12,359 rows had one of our two target address values (only 3,558 with a street address and 1,321 with a zip code), we omitted addresses from matching in this set to avoid the undue influence of missingness in these values.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

checking <- aps |>
  filter(chunks == 4)

# How many rows are in this subset?
format(nrow(
  checking
  ), big.mark = ',')

# How many rows have a street address?
format(nrow(
  checking |>
    filter(!is.na(client_street_address))
  ), big.mark = ',')

# How many rows have a zip code?
format(nrow(
  checking |>
    filter(!is.na(client_zip_code))
  ), big.mark = ',')

# [1] "12,359"
# [1] "3,558"
# [1] "1,321"
```

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name"
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )

# Set seed
set.seed(doc_seed)

subset <- aps |>
  dplyr::filter(chunks == 4)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_4'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars, threshold.match = 0.20
  )
```

##### 💾 Save and Export Data

We exported our data, for further use.

```{r}
saveRDS(
  fl_out,
  here::here(
    "data", "cleaned_rds_files", "unique_subject_ids", "aps", 
    "aps_01_fl_chunk_4.rds"
    )
)

```

#### Chunk 5

Chunk 5 represented rows which had missing dates of birth. As these values were either all present or entirely absent due to how these values were input in the original data set, we omitted dates of birth from matching in this set to avoid the undue influence of missingness in these values.

```{r}
checking <- aps |>
  filter(chunks == 5)

# How many rows are in this subset?
format(nrow(
  checking
  ), big.mark = ',')
# [1] "28"
```

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", "client_street_address"
  )
num_vars <- c(
  "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )

# Set seed
set.seed(doc_seed)

subset <- aps |>
  dplyr::filter(chunks == 5)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_5'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars, threshold.match = 0.20
  )
```

##### 💾 Save and Export Data

We exported our data, for further use.

```{r}
saveRDS(
  fl_out,
  here::here(
    "data", "cleaned_rds_files", "unique_subject_ids", "aps", 
    "aps_01_fl_chunk_5.rds"
    )
)

```

### Chunk 6

Chunk 6 represented rows which had missingness in more than one field. While nearly all of these entries had a date of birth, we could not identify any other of our identifiers present in more than half of the chunk. However, it's small size would make manual review relatively reasonable, so all identifiers were utilized for matching within this set.

```{r}
## ==========================================================================
## This chunk is meant to have multiple outputs, used by a human using the
## file to perform a standardized set of data checks in one block.
## ==========================================================================

checking <- aps |>
  filter(chunks == 6)

# How many rows are in this subset?
format(nrow(
  checking
  ), big.mark = ',')

# How many rows have first name?
format(nrow(
  checking |>
    filter(!is.na(client_first_name))
  ), big.mark = ',')

# How many rows have last name?
format(nrow(
  checking |>
    filter(!is.na(client_last_name))
  ), big.mark = ',')

# How many rows have a street address?
format(nrow(
  checking |>
    filter(!is.na(client_street_address))
  ), big.mark = ',')

# How many rows have a zip code?
format(nrow(
  checking |>
    filter(!is.na(client_zip_code))
  ), big.mark = ',')

# How many rows have date of birth?
format(nrow(
  checking |>
    filter(!is.na(client_date_of_birth))
  ), big.mark = ',')

# [1] "549"
# [1] "196"
# [1] "33"
# [1] "67"
# [1] "31"
# [1] "526"
```

```{r}
## Defining Target Variables
str_vars <- c(
  "client_first_name", "client_last_name", "client_street_address"
  )
num_vars <- c(
  "client_dob_year", "client_dob_month", "client_dob_day", "client_zip_code"
  )

## Defining packed identifiers
aps_pack$ids <- c(
  'aps_row', str_vars, num_vars, 'client_id', 'client_notes',
  'flag_unresolvable', 'flag_mult_clients', 'flags'
  )

# Set seed
set.seed(doc_seed)

subset <- aps |>
  dplyr::filter(chunks == 6)

## Packing Subset
subset_pack <- list()
subset_pack$df <- subset
subset_pack$suffix <- 'aps_6'
subset_pack$ids <- aps_pack$ids

## fastLink
fl_out <- single_df_fastLink(
  subset, str_vars, num_vars, partial.match = str_vars, threshold.match = 0.20
  )
```

#### 💾 Save and Export Data

We exported our data, for further use.

```{r}
saveRDS(
  fl_out,
  here::here(
    "data", "cleaned_rds_files", "unique_subject_ids", "aps", 
    "aps_01_fl_chunk_6.rds"
    )
)

```

## Consolidation of APS flags

We modified our APS set so that the "flag_issues" column would be "TRUE" if the row displayed missingness (chunk > 2), or was flagged previously as either unresolvable or associated with a `case_id` that had more than one `client_id` value (representing a likely failed match in `client_id` values).

```{r}
aps <- aps |>
  dplyr::mutate(
    flag_issues = dplyr::case_when(
      flag_unresolvable ~ T,
      flag_mult_clients ~ T,
      chunks > 2 ~ T,
      TRUE ~ F
    )
  )
```


### 💾 Save and Export Data

We exported our APS data, including row numbers, chunk assignments, and flags, in case it was required later.

```{r}
saveRDS(
  aps,
  here::here(
    "data", "cleaned_rds_files", "unique_subject_ids", "aps", 
    "aps_01_prepped_for_fl.rds"
    )
)

```

# 🧹 Clean up

```{r}
rm(list=ls())
```
